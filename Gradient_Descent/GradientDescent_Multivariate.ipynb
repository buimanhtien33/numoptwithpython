{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfqhL+6cGts4zm93lJj3+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buimanhtien33/numoptwithpython/blob/main/Gradient_Descent/GradientDescent_Multivariate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent method for multivariable function, where gradient function is created with grad from autograd"
      ],
      "metadata": {
        "id": "XyxYY3z_9HXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XokWJTTgcRBA"
      },
      "outputs": [],
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "\n",
        "import sympy as sp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return x[0]**2/2 + 5*x[1]**2 + 2*(x[0] - 2)**4/3 + 8*(x[1] + 1)**4"
      ],
      "metadata": {
        "id": "DHVjcwJcvlK-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test gradient by using grad\n",
        "df = grad(f)\n",
        "x = np.array([1., 2.])\n",
        "df(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXzkOfoLxgJo",
        "outputId": "bb654373-1c7b-4bff-e3f6-ba1b26a619e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -1.66666667, 884.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_multi(cost_fn, initial_guess, step_size, precision, n_max):\n",
        "  grad_f = grad(cost_fn)\n",
        "  new_x = initial_guess\n",
        "\n",
        "  for n in range(n_max):\n",
        "    print('iter: ',n)\n",
        "    #step 1: predict (make a guess)\n",
        "    previous_x = new_x\n",
        "    \n",
        "    #step 2: calculate the gradient\n",
        "    gradval = grad_f(previous_x)\n",
        "\n",
        "    #step 3: Learn (make ajustments)\n",
        "    new_x = previous_x - step_size*gradval\n",
        "    print('New_x = ', new_x)\n",
        "\n",
        "    #step 4: check convergence condition\n",
        "    step_length = (new_x - previous_x).dot(new_x - previous_x)\n",
        "    print('Step length= ',step_length)\n",
        "    if step_length < precision:\n",
        "      break\n",
        "    \n",
        "  print('Minimum occurs at x-value: ', new_x)\n",
        "\n",
        "  return new_x"
      ],
      "metadata": {
        "id": "8X0wMG-dxvAb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0_guess = np.array([2., 1.])\n",
        "step_size = 0.01\n",
        "precision = 0.01\n",
        "iter_max = 100\n",
        "x_opt = gradient_descent_multi(f, x0_guess, step_size, precision, iter_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw16abw88JL_",
        "outputId": "8eb29584-83fe-4e29-fe8b-e72d5a0b8f2b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter:  0\n",
            "New_x =  [ 1.98 -1.66]\n",
            "Step length=  7.0760000000000005\n",
            "iter:  1\n",
            "New_x =  [ 1.96020021 -1.40200128]\n",
            "Step length=  0.06695537107368392\n",
            "iter:  2\n",
            "New_x =  [ 1.94059989 -1.24101221]\n",
            "Step length=  0.026301651676498086\n",
            "iter:  3\n",
            "New_x =  [ 1.92119948 -1.11243111]\n",
            "Step length=  0.01690947757941501\n",
            "iter:  4\n",
            "New_x =  [ 1.90200054 -1.00073321]\n",
            "Step length=  0.012845020186227232\n",
            "iter:  5\n",
            "New_x =  [ 1.88300563 -0.90065989]\n",
            "Step length=  0.010375476036113877\n",
            "iter:  6\n",
            "New_x =  [ 1.86421828 -0.8109076 ]\n",
            "Step length=  0.008408436706804915\n",
            "Minimum occurs at x-value:  [ 1.86421828 -0.8109076 ]\n"
          ]
        }
      ]
    }
  ]
}